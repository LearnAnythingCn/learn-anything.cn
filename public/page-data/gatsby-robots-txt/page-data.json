{
    "componentChunkName": "component---src-templates-article-article-detail-gen-js",
    "path": "/gatsby-robots-txt",
    "result": {"data":{"strapiArticles":{"id":"Articles_353","title":"使用robots.txt禁止搜索引擎爬虫访问网站！","description":"robots.txt（统一小写）是一种存放于网站根目录下的文本文件(ASCII编码)，告诉网络搜索引擎的爬虫，哪些内容可以访问，哪些内容禁止访问。\nrobots.txt协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。","content":"##### 一、简介\n\nrobots.txt（统一小写）是一种存放于网站根目录下的文本文件(ASCII编码)，告诉网络搜索引擎的爬虫，哪些内容可以访问，哪些内容禁止访问。\n\n\nrobots.txt协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。\n\n---\n\n##### 二、Gatsby 中使用 robots.txt\n\n###### 1、安装\n\n```shell\nnpm install --save gatsby-plugin-robots-txt\n# or\nyarn add gatsby-plugin-robots-txt\n```\n\n###### 2、配置\n\n在  gatsby-config.js  文件中配置，不对本站内容做任何限制，搜索引擎爬虫可以访问任何页面。\n\n```js\n// gatsby-config.js\n\nmodule.exports = {\n  plugins: [\n    {\n      resolve: 'gatsby-plugin-robots-txt',\n      options: {\n        host: 'https://learn-anything.cn',\n        sitemap: 'https://learn-anything.cn/sitemap-pages.xml',\n        policy: [{ userAgent: '*', allow: '/' }]\n      }\n    }\n  ]\n};\n```\n\n###### 3、访问\n\n重新启动 gatsby 项目，即可查看 robots.txt 文件。链接：http://localhost:8000/robots.txt\n\n---\n\n##### 三、其他配置说明\n\n```shell\n# 允许特定的机器人：（name_spider用真实名字代替）\nUser-agent: name_spider\nAllow:\n\n# 拦截所有爬虫访问所有页面\nUser-agent: *\nDisallow: /\n\n# 禁止所有爬虫访问特定目录：\nUser-agent: *\nDisallow: /cgi-bin/\nDisallow: /images/\n\n# 禁止坏爬虫访问特定目录\nUser-agent: BadBot\nDisallow: /private/\n\n# 禁止所有爬虫访问特定文件类型\nUser-agent: *\nDisallow: /*.php$\nDisallow: /*.js$\n```\n\n---\n\n##### 四、参考文档\n\n- [gatsby-plugin-robots-txt](https://www.gatsbyjs.com/plugins/gatsby-plugin-robots-txt)\n\n\n","url":"https://learn-anything.cn/gatsby-robots-txt","published_at":"2021-11-11T03:42:05.000Z","tags":[{"name":"Gatsby","slug":"gatsby"}]},"site":{"siteMetadata":{"title":"收集优质资源","author":"@learn-anything.cn","description":"追求极简语言，说明事物因果！","email":"lh@learn-anything.cn"}}},"pageContext":{"title":"使用robots.txt禁止搜索引擎爬虫访问网站！","slug":"gatsby-robots-txt"}},
    "staticQueryHashes": ["63159454"]}